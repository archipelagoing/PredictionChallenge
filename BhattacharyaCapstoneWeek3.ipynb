{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **<font color='##32a852'>Assignment 3: Prediction </font>**\n","\n","\n","### **<font color='###6b32a8'>Model accuracy on validation data: 93.26% </font>**\n","\n","##Archisa Bhattacharya"],"metadata":{"id":"x_lubFlrZqgn"}},{"cell_type":"markdown","source":["**PROJECT LINKS:**\n","\n","Kaggle Dataset: https://www.kaggle.com/datasets/lainguyn123/student-performance-factors\n","\n","---"],"metadata":{"id":"cQshMJsoaKAQ"}},{"cell_type":"markdown","source":["# **<font color='violet'>Libraries</font>**"],"metadata":{"id":"BkW7yJPKdaQK"}},{"cell_type":"code","source":["from google.colab import files\n","import numpy as np\n","import pandas as pd\n","import io\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n"],"metadata":{"id":"g_bRAcxUZpkq","executionInfo":{"status":"ok","timestamp":1727673089529,"user_tz":240,"elapsed":3870,"user":{"displayName":"Archisa Bhattacharya","userId":"00711226775337108328"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# **<font color='violet'>Part 1:</font> Loading the Data, EDA, & Data Cleaning**"],"metadata":{"id":"r3Iy1aTXdiiD"}},{"cell_type":"markdown","source":["## **<font color='green'>Loading the Data </font>**"],"metadata":{"id":"_O8Iol30dujX"}},{"cell_type":"code","source":["# Mount Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Dq48FnIwdj4r","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"e9eff020-a948-46b9-b039-08bde65919ce","executionInfo":{"status":"ok","timestamp":1727673959913,"user_tz":240,"elapsed":1382,"user":{"displayName":"Archisa Bhattacharya","userId":"00711226775337108328"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Read in data path\n","\n","training_data = pd.read_csv('/content/drive/My Drive/DSAssignment3/PCTrainUpdated.csv')\n","testing_data = pd.read_csv('/content/drive/My Drive/DSAssignment3/PCTestUpdated.csv')\n","conversion_rates = pd.read_csv('/content/drive/My Drive/DSAssignment3/currency_conversion_rates.csv')\n"],"metadata":{"id":"hP7Ow45HSMt-","executionInfo":{"status":"ok","timestamp":1727674096146,"user_tz":240,"elapsed":4916,"user":{"displayName":"Archisa Bhattacharya","userId":"00711226775337108328"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["Lets try to compare all of the different data sets we are considering and see if we can find any interesting patterns in the data visually, through a heatmap of the correlation matrix. This will let us easily see which values appear to be correlated to each other. While correlation does not equal causation, this will help us have a nice jumping-off point!\\"],"metadata":{"id":"8Hj-Q8ELFFop"}},{"cell_type":"code","source":["# Currency Conversion method\n","# Convert price to USD using conversion rates\n","def convToUSD(price, convRates):\n","    if isinstance(price, str):\n","        try:\n","            price_value, currency = price.split() # split the 'price' into its value and the currency it uses\n","            price_value = float(price_value)\n","            convRate = convRates.get(currency, 1.0)  # Convert the currency to USD, if there is no currency symbol being used, assume it is already USD\n","            return price_value / convRate\n","        except:\n","            return None     # If a conversion couldn't be  performed, return None , otherwise, return the price\n","    return price\n","\n","currDict = dict(zip(conversion_rates['Currency_Code'], conversion_rates['100_USD_worth']))\n","\n","# Apply currency conversion\n","training_data['actual_price_usd'] = training_data['actual_price'].apply(lambda x: convToUSD(x, currDict))\n","training_data['discount_price_usd'] = training_data['discount_price'].apply(lambda x: convToUSD(x, currDict))\n","\n","testing_data['actual_price_usd'] = testing_data['actual_price'].apply(lambda x: convToUSD(x, currDict))\n","testing_data['discount_price_usd'] = testing_data['discount_price'].apply(lambda x: convToUSD(x, currDict))\n"],"metadata":{"id":"RV4_yeVZTKjN","executionInfo":{"status":"ok","timestamp":1727675659939,"user_tz":240,"elapsed":1391,"user":{"displayName":"Archisa Bhattacharya","userId":"00711226775337108328"}}},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":["# CLEANING THE MISSING VALUES\n","\n","There is still a lot of missing values in our training data frame. Dropping them would be much easier. However, if we simply drop them, we could lose out on a lot of precious data, and this could make our final prediction model much weaker. Lets fill it in with an average value. I chose the median instead of the mean because if there are any outliers then it will ignore them. We will compute the discount percentt & add it to a new column in our training data.\n",""],"metadata":{"id":"B2MJSPfhfp8h"}},{"cell_type":"markdown","source":["This is called Feature Engineering in Machine Learning!! It is an invaluable way to manipulate the data that we have already collected to make it more useful to us. In this case, we use a simple percentage equation to  calculate the discount percentage using hte 2 features, actual price & discount price.\n","You may note that from the original features, we have performed, in order\n","\n","\n","1.   Converting the features we have to a usable currency (USD)\n","2.   Systematically cleaning the data we have with a certain equation that best suits our purpose\n","3. Finding the missing values in our data & filling it in with a specific value.\n","\n","\n","\n","I find that this process is very similar to the process of knitting, from start to finish.\n","1. First, you must make the yarn usable. When you first buy yarn, it is very difficult to keep the yarn skein in place. Many knitters have to re-wind the skein of yarn into a ball of yarn, so that they can pull the yarn from the center. This makes it such that the yarn will not move around & create hassles while crocheting.\n","2. Second, you must determine the best stitch to use to complete your piece. There are many different varieties and types of knitting. A simple stitch is called the 'knit' stitch, which involves manipulating 'data' (yarn) using two 'vectors' (knitting needles). Just like choosing the appropriate method of cleaning, choosing the right stitch is vital to finishing a piece most quickly and efficiently using the amount of yarn provided.\n","3. Once you have finished knitting, you may notice that you have made many human errors, such as dropping stitches, which can leave large holes that can cause the entire piece to unravel. Filling these holes with the right kind of stitch is vital to ensuring the quality of the final product.\n","\n"],"metadata":{"id":"z15w96f6ie-a"}},{"cell_type":"markdown","source":["# PICKING FEATURES FOR OUR MODEL\n","\n","Now comes the hard part: We have to decide what features we want in the model. Luckily, we have a couple hints provided to us on what features to use.\n","\n","Hint: discount% (should be straightforward), discount_amount (should be straightforward), weighted_rating (x + ay^2 where x and y are other columns, and a is a constant)\n","\n","I cleaned the Ratings and # of Ratings columns, to ensure that all empty or null values were filled in with the best measure of center (again, using median to prevent overweighting any outliers)\n","\n","Then, much like knitting, I chose the appropriate equation & performed the creation & cleaning of these features, as adviced.\n"],"metadata":{"id":"TMup58ZNoj8z"}},{"cell_type":"code","source":["training_data_clean = training_data_clean.copy()\n","testing_data_clean = testing_data_clean.copy()\n","\n","######  RATINGS & # OF RATINGS ############\n","# Clean Ratings & # of Ratings Features\n","training_data_clean.loc[:, 'ratings'] = training_data_clean['ratings'].fillna(training_data_clean['ratings'].median())\n","training_data_clean.loc[:, 'no_of_ratings'] = training_data_clean['no_of_ratings'].fillna(0)\n","# testing_data_clean.loc[:, 'ratings'] = testing_data_clean['ratings'].fillna(training_data_clean['ratings'].median())\n","# testing_data_clean.loc[:, 'no_of_ratings'] = testing_data_clean['no_of_ratings'].fillna(0)\n","\n","######  DISCOUNT PERCENT ############\n","\n","# Create & CLean the 'discount_percent' feature\n","training_data_clean.loc[:, 'discount_percent'] = 100 * (training_data_clean['actual_price_usd'] - training_data_clean['discount_price_usd']) / training_data_clean['actual_price_usd']\n","# testing_data_clean.loc[:, 'discount_percent'] = 100 * (testing_data_clean['actual_price_usd'] - testing_data_clean['discount_price_usd']) / testing_data_clean['actual_price_usd']\n","\n","######  DISCOUNT AMOUNT ############\n","# Create & CLean the 'discount_amount' feature, in training & testing\n","training_data_clean.loc[:, 'discount_amount'] = training_data_clean['actual_price_usd'] - training_data_clean['discount_price_usd']\n","#testing_data_clean.loc[:, 'discount_amount'] = testing_data_clean['actual_price_usd'] - testing_data_clean['discount_price_usd']\n","\n","######  WEIGHTED RATING ############\n","# Define a constant 'a' for weighted_rating (you can tune this later)\n","a = 0.1\n","\n","# Create & CLean the 'weighted_rating' feature, in training & testing\n","training_data_clean.loc[:, 'weighted_rating'] = training_data_clean['ratings'] + a * (training_data_clean['no_of_ratings'] ** 2)\n","#testing_data_clean.loc[:, 'weighted_rating'] = testing_data_clean['ratings'] + a * (testing_data_clean['no_of_ratings'] ** 2)\n","\n","print(training_data_clean[['discount_amount', 'weighted_rating']].head())\n","#print(testing_data_clean[['discount_amount', 'weighted_rating']].head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"msUzwDtglmSa","executionInfo":{"status":"ok","timestamp":1727678503458,"user_tz":240,"elapsed":216,"user":{"displayName":"Archisa Bhattacharya","userId":"00711226775337108328"}},"outputId":"15e4546f-0c1e-49a5-bd85-89440a3b4e1b"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["   discount_amount  weighted_rating\n","0         0.152195              6.6\n","2         0.005955              3.9\n","3         0.527068         606640.7\n","4         1.927801              5.4\n","5         2.703109              3.9\n"]}]},{"cell_type":"markdown","source":["# TRAINING THE MODEL\n","\n","Now it is time to train the model! I use the feature which I cleaned already & select them from the training data. I have created 2 variables, X and Y. X is meant to be the feature matrix, which contains all of the features, as they were selected from the training data\n","\n","*    X is the feature matrix, which contains all of the features, as they were selected from the training data. This works like a 'checklist' to see which features we want to use together.\n","*  Y has all of the values of the 'purchase?' column.\n","\n","Then I used SciKit Learn to split my data into training & validation sets. While testing, I tried many different values, but for submission, 80% of my data is for training and 20% is for validation. Another technique that can be used to split data into testing and training data is called k-fold cross validation! This one reminds me of the process of kneading a loaf of dough for bread :)\n","\n","Finaly, I train the DecisionTreeClassifier from SciKit Learn & generate my predictions.\n"],"metadata":{"id":"_iVdvcR8pwFt"}},{"cell_type":"code","source":["features = ['discount_percent', 'ratings', 'no_of_ratings', 'discount_amount', 'weighted_rating']\n","\n","X = training_data_clean[features]\n","\n","y = training_data_clean['purchase?'].apply(lambda x: 1 if x == 'YES' else 0)"],"metadata":{"id":"v-POJTQnmfnd","executionInfo":{"status":"ok","timestamp":1727677521647,"user_tz":240,"elapsed":272,"user":{"displayName":"Archisa Bhattacharya","userId":"00711226775337108328"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","model = DecisionTreeClassifier(random_state=42)\n","model.fit(X_train, y_train)\n","\n","y_pred = model.predict(X_val)\n","accuracy = accuracy_score(y_val, y_pred)\n","print(f\"Model accuracy on validation data: {accuracy * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"rleuhyf3miWQ","executionInfo":{"status":"ok","timestamp":1727677589754,"user_tz":240,"elapsed":3233,"user":{"displayName":"Archisa Bhattacharya","userId":"00711226775337108328"}},"outputId":"8cdc8b8b-a8d2-45cc-bf99-d162768b9d41"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Model accuracy on validation data: 93.27%\n"]}]},{"cell_type":"code","source":["X_test = testing_data_clean[features]\n","test_predictions = model.predict(X_test)\n","\n","testing_data_clean.loc[:, 'purchase?'] = test_predictions\n","testing_data_clean.loc[:, 'purchase?'] = testing_data_clean['purchase?'].apply(lambda x: 'YES' if x == 1 else 'NO')\n","\n","testing_data_clean[['item_id', 'purchase?']].to_csv('/content/drive/My Drive/DSAssignment3/predicted_purchases2.csv', index=False)"],"metadata":{"id":"7JCPFVOompTr","executionInfo":{"status":"ok","timestamp":1727677530125,"user_tz":240,"elapsed":283,"user":{"displayName":"Archisa Bhattacharya","userId":"00711226775337108328"}}},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":["--------------"],"metadata":{"id":"ibGNLVNBmgQU"}}]}